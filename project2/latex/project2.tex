\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}


\title{\textbf{FYS4150/FYS1350 - Project 2}}
\author{Ingvild Bergsbak, Oliver Hebnes and Erlend Ousdal}
\date{October 1}




\begin{document}

\maketitle

\newpage

\section{Abstract}


\section{Introduction}


\section{Theoretical Models and Technicalities}

We have av orthogonal matrix so that
$$\mathbf{v}_j^T\mathbf{v}_i=\delta_{ij}$$
and othogonal transformation $\mathbf{w}$.\\
\vskip0.1cm
$\mathbf{w}_i=\mathbf{Uv}_i$\\
\vskip0.1cm
$\mathbf{w}_j^T\cdot \mathbf{w}_i=(\mathbf{Uv}_j)^T\cdot (\mathbf{Uv}_i)$
\vskip0.5cm
Calculating $\mathbf{Uv}_i$ and $(\mathbf{Uv}_j)^T$ separately.
\begin{equation*}
\mathbf{Uv}_i=\begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n}\\
u_{21} & u_{22} & \cdots & u_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
u_{n1} & u_{n2} & \cdots & u_{nn}\\
\end{bmatrix} \begin{bmatrix}
v_{i1} \\
v_{i2} \\
\vdots \\
v_{in} \\
\end{bmatrix}=\begin{bmatrix}
u_{11}v_{i1} + u_{12}v_{i2} + \cdots + u_{1n}v_{in}\\
u_{21}v_{i1} + u_{22}v_{i2} + \cdots + u_{2n}v_{in}\\
\vdots \\
u_{n1}v_{i1} +  u_{n2}v_{i2} + \cdots + u_{nn}v_{in}\\
\end{bmatrix}
\end{equation*}



\begin{equation*}
\begin{split}
(\mathbf{Uv}_j)^T&=\begin{bmatrix}
u_{11}v_{j1} + \cdots + u_{1n}v_{jn} &
u_{21}v_{j1} +  \cdots + u_{2n}v_{jn} &
\cdots &
u_{n1}v_{j1} + \cdots + u_{nn}v_{jn}\\
\end{bmatrix}\\
&=\begin{bmatrix}
v_{j1} &
v_{j2} &
\cdots &
v_{jn}
\end{bmatrix}\begin{bmatrix}
u_{11} & u_{21} & \cdots & u_{n1}\\
u_{12} & u_{22} & \cdots & u_{n2}\\
\vdots & \vdots & \ddots & \vdots\\
u_{1n} & u_{2n} & \cdots & u_{nn}\\
\end{bmatrix} \\
&=\mathbf{v}_j^T\mathbf{U}^T
\end{split}
\end{equation*}

$\mathbf{Uv}_i$ and $(\mathbf{Uv}_j)^T$ inserted back into the equation gives

\begin{equation*}
\begin{split}
\mathbf{w}_j^T\cdot \mathbf{w}_i&=(\mathbf{Uv}_j)^T\cdot (\mathbf{Uv}_i)\\
&=\mathbf{v}_j^T\mathbf{U}^T\mathbf{Uv}_i\\
&=\mathbf{v}_j^T\mathbf{I}\mathbf{v}_i\\
&=\mathbf{v}_j^T\mathbf{v}_i\\
&=\delta_{ij}
\end{split}
\end{equation*}

The orthogonal transformation preserves orthogonality and the dot product.














\section{Results and Discussion}

For a given tolerance (we used eps$=10^{-15})$) we estimated the number of transformations found in figure \ref{simtrans}. If the dimension of a matrix increases, so will the number of similarity transformations.
But the scale is not linear, which means we should not neccesarily use this Jacobi's rotation algorithm for very big matrices because of the number of similarity transformations will increase with the dimensionality of the matrix.
If we time this with the number of flops in Jacobi's method (), it's easy to understand why we can draw this conclusion, and why it takes it very long time computing the eigenvalues, as seen in table \ref{timeusednumpy} and table \ref{timeusedjacobi}.


\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|r|}
    \hline
     run & n=10 & n=20 & n=30\\
     \hline
      1  & 0.1043 s & 1.8843 s & 9.6493 s\\
      2  & 0.1004 s & 1.8887 s & 9.8702 s\\
      3  & 0.1053 s & 1.9206 s & 9.8153 s\\
      4  & 0.1027 s & 1.8263 s & 9.4729 s\\
      5  & 0.1002 s & 1.7854 s & 9.6888 s\\
      6  & 0.1053 s & 1.8869 s & 9.5218 s\\
      7  & 0.1014 s & 1.7886 s & 9.8120 s\\
      8  & 0.1052 s & 1.8935 s & 9.8537 s\\
      9  & 0.1094 s & 1.8524 s & 9.7283 s\\
      10 & 0.1002 s & 1.8545 s & 9.4581 s\\
      \hline
      Average & 0.10344 s & 1.85812  s & 9.68704 s\\
      \hline
    \end{tabular}
    \caption{Elapsed time for Jacobi's rotation method in Python with n equal the matrix dimension}
    \label{timeusedjacobi}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|r|}
    \hline
     run & n=10 & n=20 & n=30 \\
     \hline
      1  & 0.000333 s & 0.000324 s & 0.000370 s\\
      2  & 0.000328 s & 0.000320 s & 0.000414 s\\
      3  & 0.000247 s & 0.000329 s & 0.000335 s\\
      4  & 0.000239 s & 0.000279 s & 0.000332 s\\
      5  & 0.000244 s & 0.000328 s & 0.000305 s\\
      6  & 0.000240 s & 0.000583 s & 0.000312 s\\
      7  & 0.000252 s & 0.000362 s & 0.000400 s\\
      8  & 0.000265 s & 0.000268 s & 0.000370 s\\
      9  & 0.000240 s & 0.000322 s & 0.000314 s\\
      10 & 0.000242 s & 0.000276 s & 0.000304 s\\
      \hline
      Average & 0.000263 s & 0.000339 s & 0.000346 s\\
      \hline
    \end{tabular}
    \caption{Elapsed time for Numpy's eigenvalue function in Python with n equal the matrix dimension.}
    \label{timeusednumpy}
\end{table}



\begin{figure}
  \includegraphics{simtransformations.eps}
  \caption{Plot of the number of similarity transformations given by a tolerance of $10^{-15}$ without extending to quantum mechanics.}
  \label{simtrans}
\end{figure}




























\end{document}



\begin{bmatrix}
1 & 1 & 0 & 2\\
0 & 1 & 0 & 1\\
1 & 0 & 0 & 1\\
1 & 0 & 1 & 2\\
\end{bmatrix}
